{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Interpolation methods\n",
    "\n",
    "    Bilinear : takes the weighted average of four nearest pixels in the original image \n",
    "               to compute the value of a new pixel in the scaled image\n",
    "\n",
    "    Nearest-neighbor: picks the pixel value of the nearest pixel in the input image and\n",
    "                      uses it for the output pixel. It is the fastest but results in\n",
    "                      blocky or pixelated output.\n",
    "\n",
    "    Bicubic interpolation: This method is more computationally expensive than bilinear \n",
    "                           interpolation and is known to produce smoother output. \n",
    "                           It takes a weighted average of 16 surrounding pixels \n",
    "                           in a 4x4 pixel neighborhood of the input image.\n",
    "\n",
    "    Lanczos interpolation: This method is a more advanced version of bicubic interpolation \n",
    "                           that is known to produce higher quality output\n",
    "                           but is even more computationally expensive.\n",
    "    \n",
    "    interpolation\n",
    "        0 -> Nearest-neighbor interpolation\n",
    "        1 -> Bilinear interpolation\n",
    "        2 -> Bicubic interpolation\n",
    "        3 -> Lanczos interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderate_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/MODERATE-GLAUCOMA'\n",
    "no_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/NO-GLAUCOMA'\n",
    "severe_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/MODERATE-GLAUCOMA'\n",
    "target_size = 224\n",
    "interpolation_type = 2\n",
    "normalization_mean = (0.5, 0.5, 0.5)\n",
    "normalization_std = (0.5, 0.5, 0.5)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_transform = transforms.Resize((target_size, target_size), interpolation=interpolation_type)\n",
    "tensor_transform = transforms.ToTensor()\n",
    "normalize_transform = transforms.Normalize(normalization_mean, normalization_std)\n",
    "transform = transforms.Compose([\n",
    "            resize_transform,\n",
    "            tensor_transform,\n",
    "            normalize_transform\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('./data/MODERATE-GLAUCOMA/N116231_20170615_125121_Color_R_001.JPG')\n",
    "image_tensor = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFolderLoader(Dataset):\n",
    "    def __init__(self, image_dir, label, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(object):\n",
    "    def __init__(self, target_size_height, target_size_width):\n",
    "        self.moderate_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/MODERATE-GLAUCOMA/'\n",
    "        self.no_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/NO-GLAUCOMA/'\n",
    "        self.severe_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/SEVERE-GLAUCOMA/'\n",
    "        self.target_size_height = target_size_height\n",
    "        self.target_size_width = target_size_width\n",
    "        self.interpolation_type = 2\n",
    "        self.normalization_mean = (0.5, 0.5, 0.5)\n",
    "        self.normalization_std = (0.5, 0.5, 0.5)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((self.target_size_height, self.target_size_width), interpolation = self.interpolation_type),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.normalization_mean, self.normalization_std)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def load_data(self, label, folder):\n",
    "        data = DatasetFolderLoader(folder, label, self.transform)\n",
    "        train_size = int(0.8 * len(data))\n",
    "        test_size = len(data) - train_size\n",
    "        train_data, test_data = random_split(data, [train_size, test_size])\n",
    "        return train_data, test_data\n",
    "    \n",
    "    def load_customdataset(self):\n",
    "        \"\"\" 0: moderate_glucoma, 1: no_glucoma, 2: severe_glucoma\"\"\"\n",
    "        moderate_glucoma_train, moderate_glucoma_test = self.load_data(0, self.moderate_glucoma_folder)\n",
    "        no_glucoma_train, no_glucoma_test = self.load_data(1, self.no_glucoma_folder)\n",
    "        severe_glucoma_train, severe_glucoma_test = self.load_data(2, self.severe_glucoma_folder)\n",
    "        all_training_data = torch.utils.data.ConcatDataset([moderate_glucoma_train, no_glucoma_train, severe_glucoma_train])\n",
    "        all_testing_data = torch.utils.data.ConcatDataset([moderate_glucoma_test, no_glucoma_test, severe_glucoma_test])\n",
    "        return all_training_data, all_testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train, all_test = MyCustomDataset(224, 224).load_customdataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_train), len(all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCustomCNN, self).__init__()\n",
    "        self.input_depth = 3\n",
    "        self.output_classes = 3\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(10, 6, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(6 * 112 * 112, 120),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(120, 10),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(10, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(x.size())\n",
    "        \"\"\"\n",
    "            After the first convolutional layer: \n",
    "                output size = (input size - kernel size + 2 * padding) / stride + 1 \n",
    "                            = (224 - 3 + 2) / 1 + 1 = 224 x 224  x 10\n",
    "            After the second convolutional layer: \n",
    "                output size = (input size - kernel size + 2 * padding) / stride + 1 \n",
    "                            = (224 - 3 + 2) / 1 + 1 = 224 x 224 x 6\n",
    "            After the maxpool layer: \n",
    "                output size = (input size - kernel size) / stride + 1 \n",
    "                            = (224 - 2) / 2 + 1 = 112 x 112 x 6\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 6 * 112 * 112)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyCustomCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data, all_test_data = MyCustomDataset(224, 224).load_customdataset()\n",
    "\n",
    "train_loader = DataLoader(all_train_data, batch_size=30, shuffle=True)\n",
    "# Create the custom CNN model\n",
    "model = MyCustomCNN().to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Define the path to save the model\n",
    "model_path = 'my_model.pt'\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(all_test_data, batch_size=30)\n",
    "\n",
    "# Load the trained model\n",
    "model = MyCustomCNN()\n",
    "model.load_state_dict(torch.load('my_model.pth'))\n",
    "\n",
    "# Test the model on the test dataset\n",
    "test_acc = test(model, test_loader)\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the true and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Iterate through the test dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Add the true and predicted labels to the lists\n",
    "        true_labels += labels.tolist()\n",
    "        pred_labels += predicted.tolist()\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
